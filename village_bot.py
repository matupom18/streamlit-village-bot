from langchain.schema import HumanMessage, SystemMessage
from langchain_community.document_loaders import TextLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from chromadb.config import Settings  
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain_google_genai import ChatGoogleGenerativeAI
import mysql.connector
import streamlit as st
import os
# ‡πÇ‡∏´‡∏•‡∏î Google API Key ‡∏à‡∏≤‡∏Å secrets.toml
os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]

# --- Database connection ---
ssl_path = os.path.join(os.path.dirname(__file__), "isrgrootx1.pem")
conn = mysql.connector.connect(
    host=st.secrets["DB_HOST"],
    port=st.secrets["DB_PORT"],
    user=st.secrets["DB_USER"],
    password=st.secrets["DB_PASSWORD"],
    database=st.secrets["DB_NAME"],
    ssl_ca=ssl_path,
    ssl_verify_cert=True,
    ssl_verify_identity=True
)
cursor = conn.cursor()

# --- Load and process documents ---
from pathlib import Path

def load_txt_documents(directory):
    docs = []
    for file_path in Path(directory).rglob("*.txt"):
        loader = TextLoader(str(file_path), encoding="utf-8")
        docs.extend(loader.load())
    return docs

docs = load_txt_documents("village_docs")

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = splitter.split_documents(docs)

# --- Embedding model ---
embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

# --- Vector database ---

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Chroma ‡πÅ‡∏ö‡∏ö in-memory ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
vectordb = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    client_settings=Settings(
        chroma_db_impl="duckdb+parquet",
        persist_directory=None,   # üëà ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ persist!
        anonymized_telemetry=False
    )
)


retriever = vectordb.as_retriever(search_kwargs={"k": 5})

# --- LLM setup ---
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    temperature=0.7,
    max_output_tokens=512
)

# --- Prompt setup ---
chat_prompt = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(
        "You are a helpful assistant for a housing community."
        "Answer residents' questions using information from community documents."
        "Be polite and respond in Thai."
        "You are a virtual assistant in the community."
        "You are friendly, respectful, and helpful."
        "Speak with a warm and courteous tone."
        "Always provide accurate and timely information."
        "Refer to yourself as '‡∏î‡∏¥‡∏â‡∏±‡∏ô' and use '‡∏Ñ‡πà‡∏∞/‡∏Ñ‡∏∞' appropriately.  "
        "**Do not start every message with greetings such as '‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡πà‡∏∞' unless the user greets first.**"
    ),
    HumanMessagePromptTemplate.from_template(
        "Context:\n{context}\n\nQuestion: {question}"
    ),
])

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    chain_type_kwargs={"prompt": chat_prompt},
)

# --- Public functions for Streamlit app ---

def ask_village_bot(question: str) -> str:
    """Run a query through the QA chain."""
    return qa_chain.run(question)


def is_problem_statement_with_gemini(q: str) -> bool:
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• Gemini ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡πà‡∏≤
    ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ñ‡∏≤‡∏°‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô‡∏à‡∏±‡∏î‡∏™‡∏£‡∏£‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å:
        - ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô ‡∏ô‡πâ‡∏≥‡πÑ‡∏°‡πà‡πÑ‡∏´‡∏•, ‡πÑ‡∏ü‡∏î‡∏±‡∏ö, ‡∏ñ‡∏ô‡∏ô‡∏û‡∏±‡∏á ‡∏Ø‡∏•‡∏Ø)
        - ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        - ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏à‡πâ‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        - ‡∏ú‡∏π‡πâ‡πÅ‡∏à‡πâ‡∏á (‡∏´‡∏≤‡∏Å‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó)
        
        ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥:
        - ‚Äú‡πÑ‡∏ü‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏ã‡∏≠‡∏¢ 3 ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ñ‡∏∑‡∏ô‚Äù
        - ‚Äú‡∏ñ‡∏ô‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ß‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏∏‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡πà‡∏≠‚Äù
        - ‚Äú‡∏ô‡πâ‡∏≥‡∏õ‡∏£‡∏∞‡∏õ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏´‡∏•‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡πÄ‡∏ä‡πâ‡∏≤‡πÅ‡∏ñ‡∏ß‡∏ï‡∏•‡∏≤‡∏î‡∏ô‡∏±‡∏î‚Äù
    """
    prompt = f"""
                ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ: '{q}'
                
                ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô‡∏à‡∏±‡∏î‡∏™‡∏£‡∏£‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà? 
                (‡∏ï‡∏≠‡∏ö‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏ß‡πà‡∏≤ '‡πÉ‡∏ä‡πà' ‡∏´‡∏£‡∏∑‡∏≠ '‡πÑ‡∏°‡πà')
                
                ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤:
                - ‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏ñ‡∏∂‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô ‡∏ô‡πâ‡∏≥‡πÑ‡∏°‡πà‡πÑ‡∏´‡∏• ‡πÑ‡∏ü‡∏î‡∏±‡∏ö ‡∏ñ‡∏ô‡∏ô‡πÄ‡∏™‡∏µ‡∏¢ ‡∏ô‡πâ‡∏≥‡∏ó‡πà‡∏ß‡∏° ‡∏Ø‡∏•‡∏Ø ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ '‡πÉ‡∏ä‡πà'
                - ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ '‡πÑ‡∏°‡πà'
                """
    response = llm.predict(prompt)
    return "‡πÉ‡∏ä‡πà" in response or "yes" in response.lower()


def store_issue_in_db(question: str, answer: str):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏à‡πâ‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
    query = "INSERT INTO issues (question, answer) VALUES (%s, %s)"
    values = (question, answer)
    cursor.execute(query, values)
    conn.commit()
